# -*- coding: utf-8 -*-
"""Copy of XGBOOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FjVWgWUBQnsojySttRZGdKM5N3613Ynv

***XGBOOST ALGORITHM***

---



---
"""

import pandas as pd
import numpy as np

data=pd.read_csv("/content/pd_speech_features.csv")
data.head()

"""**Preprocessing of dataset**

---


"""

data=data.drop(['id'],axis=1)
data.info()

data=data.drop_duplicates()
data=data.dropna()
data.info()

data.shape

data.columns

data.describe().transpose()

corr=data.corr()
cor_target = abs(corr["class"])
#Selecting highly correlated features
relevant_features = cor_target[cor_target>0.3]
print(relevant_features)

y=data.loc[:,'class']
x=data.iloc[:,:-1]

data=data.dropna()
data.info()

x.head()

y.head()

"""**check for unbalanced dataset**

---


"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('whitegrid')
sns.set_context('paper')
sns.set_palette('GnBu_d')
a = sns.catplot(x='class', data=data, kind='count')
a.fig.suptitle('Number of Samples in Each Class', y=1.03)
a.set(ylabel='Number of Samples', xlabel='Have Parkinson')
plt.show()

"""**Univariate Analysis**

---


"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
fig, ax = plt.subplots(1,3,figsize=(16,10))
sns.boxplot(x='f1',data=data, ax=ax[0],orient='v')
sns.boxplot(x='f2',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f3',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f4',data=data, ax=ax[1],orient='v')
sns.boxplot(x='PPE',data=data,ax=ax[2],orient='v')

"""The above figure shows the box plot of the frequency variation. All the three variations have outliers. Generally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them"""

fig, ax = plt.subplots(1,3,figsize=(16,8))
sns.distplot(data['mean_MFCC_2nd_coef'],ax=ax[0])
sns.distplot(data[  'IMF_SNR_SEO'],ax=ax[1])
sns.distplot(data[  'GNE_NSR_SEO'],ax=ax[2])

"""The measures of vocal fundamental frequency are shown above"""

fig, ax = plt.subplots(2,3,figsize=(16,8))
sns.distplot(data['locShimmer'],ax=ax[0,0])
sns.distplot(data['apq11Shimmer'],ax=ax[0,1])
sns.distplot(data['locDbShimmer'],ax=ax[0,2])
sns.distplot(data['apq3Shimmer'],ax=ax[1,0])
sns.distplot(data['apq5Shimmer'],ax=ax[1,1])
sns.distplot(data['ddaShimmer'],ax=ax[1,2])

"""For all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed"""

fig, ax = plt.subplots(1,2,figsize=(16,8))
sns.boxplot(x='class',y='DFA',data=data,ax=ax[0])
sns.boxplot(x='class',y='DFA',data=data,ax=ax[1])

# For categorical predictors
cols = ["locPctJitter","locAbsJitter","rapJitter","ppq5Jitter","ddpJitter"]
fig, axs = plt.subplots(ncols = 5,figsize=(16,8))
fig.tight_layout()
for i in range(0,len(cols)):
    sns.boxplot(x='class',y=cols[i],data=data, ax = axs[i])

"""People who are suffering for PD tend to have higher jitter %.  The variation of fundamental frequency is in a low range for people who is normal.

**Applying Model**

---
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.2)
#stratify-forcing the distribution of the target variable(s) among the different splits to be the same

for x in data.columns:
    data[x]= (data[x]-data[x].min())/(data[x].max()-data[x].min())
data.head()
y=data['class']
x=data.drop(['class'],axis=1)
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=32)

from xgboost import XGBClassifier
svc_model=XGBClassifier()
svc_model.fit(X_train,y_train)

"""**Accuracy**

---


"""

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

from sklearn.metrics import accuracy_score
print("The accuracy is",accuracy_score(y_test, y_pred))

"""**Evaluation Metrics**


---


"""

feature_imp = pd.Series(svc_model.feature_importances_,index=x.columns).sort_values(ascending=False)
feature_imp
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(predictions,y_test)
print("Precision: ",precision)
print("Recall: ",recall)
print("Thresholds: ",thresholds)

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(recall, precision, color='purple')

#add axis labels to plot
ax.set_title('Precision-Recall Curve')
ax.set_ylabel('Precision')
ax.set_xlabel('Recall')
#display plot
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test,predictions)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

from sklearn.metrics import roc_auc_score
print("ROC Accuracy score is: ",roc_auc_score(y,svc_model.predict_proba(x)[:, 1])*100,"%")

from sklearn import metrics as m
m.plot_roc_curve(svc_model, X_test, y_test)